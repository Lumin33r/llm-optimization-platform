# k8s/base/observability/grafana-dashboards.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards-provider
  namespace: observability
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
      - name: 'llm'
        orgId: 1
        folder: 'LLM Platform'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/llm
      - name: 'kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/k8s
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: observability
data:
  llm-platform.json: |
    {
      "annotations": {
        "list": []
      },
      "editable": true,
      "fiscalYearStartMonth": 0,
      "graphTooltip": 1,
      "id": null,
      "links": [],
      "panels": [
        {
          "id": 1,
          "type": "stat",
          "title": "Active Models",
          "description": "How many of the 4 vLLM models are alive. Green=4, Yellow=3, Red<3.",
          "gridPos": {
            "h": 4,
            "w": 4,
            "x": 0,
            "y": 0
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "count(up{app=~\"mistral-7b-.*\"} == 1)",
              "legendFormat": ""
            }
          ],
          "fieldConfig": {
            "defaults": {
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {
                    "color": "red",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 3
                  },
                  {
                    "color": "green",
                    "value": 4
                  }
                ]
              },
              "mappings": []
            }
          }
        },
        {
          "id": 2,
          "type": "stat",
          "title": "Total RPS",
          "description": "Platform-wide requests per second across all models.",
          "gridPos": {
            "h": 4,
            "w": 4,
            "x": 4,
            "y": 0
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "sum(rate(vllm:request_success_total{app=~\"mistral-7b-.*\"}[5m]))",
              "legendFormat": ""
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "reqps",
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  }
                ]
              }
            }
          }
        },
        {
          "id": 3,
          "type": "stat",
          "title": "P95 Latency (avg)",
          "description": "Average P95 end-to-end latency across all 4 models.",
          "gridPos": {
            "h": 4,
            "w": 4,
            "x": 8,
            "y": 0
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "avg(histogram_quantile(0.95, sum by (le,app)(rate(vllm:e2e_request_latency_seconds_bucket{app=~\"mistral-7b-.*\"}[5m]))))",
              "legendFormat": ""
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 2
                  },
                  {
                    "color": "red",
                    "value": 5
                  }
                ]
              }
            }
          }
        },
        {
          "id": 4,
          "type": "stat",
          "title": "Waiting Queue",
          "description": "Total requests waiting across all models. 0 = healthy, >0 = backpressure.",
          "gridPos": {
            "h": 4,
            "w": 4,
            "x": 12,
            "y": 0
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "sum(vllm:num_requests_waiting{app=~\"mistral-7b-.*\"})",
              "legendFormat": ""
            }
          ],
          "fieldConfig": {
            "defaults": {
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 1
                  },
                  {
                    "color": "red",
                    "value": 10
                  }
                ]
              }
            }
          }
        },
        {
          "id": 5,
          "type": "stat",
          "title": "Avg Throughput",
          "description": "Average generation throughput across all models (tokens/sec).",
          "gridPos": {
            "h": 4,
            "w": 4,
            "x": 16,
            "y": 0
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "avg(vllm:avg_generation_throughput_toks_per_s{app=~\"mistral-7b-.*\"})",
              "legendFormat": ""
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  }
                ]
              }
            }
          }
        },
        {
          "id": 6,
          "type": "stat",
          "title": "GPU Cache Avg",
          "description": "Average GPU KV-cache usage across all models. >90% = memory pressure.",
          "gridPos": {
            "h": 4,
            "w": 4,
            "x": 20,
            "y": 0
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "avg(vllm:gpu_cache_usage_perc{app=~\"mistral-7b-.*\"}) * 100",
              "legendFormat": ""
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 70
                  },
                  {
                    "color": "red",
                    "value": 90
                  }
                ]
              }
            }
          }
        },
        {
          "id": 10,
          "type": "row",
          "title": "ðŸ”¬ Quantization â€” AWQ vs FP16 Baseline",
          "gridPos": {
            "h": 1,
            "w": 24,
            "x": 0,
            "y": 4
          },
          "collapsed": false,
          "panels": []
        },
        {
          "id": 11,
          "type": "timeseries",
          "title": "P95 E2E Latency â€” AWQ vs Baseline",
          "description": "End-to-end request latency (P95). AWQ (4-bit) should be LOWER than FP16 baseline â€” faster inference from smaller weights.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 5
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-awq\"}[5m])))",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 12,
          "type": "timeseries",
          "title": "Generation Throughput â€” AWQ vs Baseline",
          "description": "Tokens generated per second. AWQ should be HIGHER than FP16 â€” 4-bit weights need less compute per token.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 5
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-awq\"}",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 13,
          "type": "timeseries",
          "title": "Time to First Token â€” AWQ vs Baseline",
          "description": "P95 prefill time. Affects perceived responsiveness. AWQ should be LOWER (smaller model = faster prefill).",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 5
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-awq\"}[5m])))",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 14,
          "type": "timeseries",
          "title": "GPU KV-Cache â€” AWQ vs Baseline",
          "description": "GPU memory for KV-cache. AWQ (4-bit) should use ~25% of FP16 cache â€” the key efficiency win.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 13
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-awq\"} * 100",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-fp16\"} * 100",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "max": 100,
              "custom": {
                "fillOpacity": 15,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 15,
          "type": "timeseries",
          "title": "Decode Time per Token â€” AWQ vs Baseline",
          "description": "P95 time to produce each output token. AWQ should be LOWER â€” faster per-token generation.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 13
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_per_output_token_seconds_bucket{app=\"mistral-7b-awq\"}[5m])))",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_per_output_token_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 16,
          "type": "timeseries",
          "title": "Tokens Generated â€” AWQ vs Baseline",
          "description": "Cumulative generation tokens. Shows volume processed â€” useful for throughput regression detection.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 13
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:generation_tokens_total{app=\"mistral-7b-awq\"}",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "vllm:generation_tokens_total{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 20,
          "type": "row",
          "title": "ðŸ§  Fine-Tuning â€” LoRA vs FP16 Baseline",
          "gridPos": {
            "h": 1,
            "w": 24,
            "x": 0,
            "y": 21
          },
          "collapsed": false,
          "panels": []
        },
        {
          "id": 21,
          "type": "timeseries",
          "title": "P95 E2E Latency â€” LoRA vs Baseline",
          "description": "LoRA adapter should add minimal latency overhead. Lines should nearly OVERLAP with FP16.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 22
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-lora\"}[5m])))",
              "legendFormat": "LoRA"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 22,
          "type": "timeseries",
          "title": "Generation Throughput â€” LoRA vs Baseline",
          "description": "LoRA throughput should be within 10% of FP16. Larger gap suggests adapter overhead.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 22
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-lora\"}",
              "legendFormat": "LoRA"
            },
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 23,
          "type": "timeseries",
          "title": "Time to First Token â€” LoRA vs Baseline",
          "description": "LoRA may have slightly higher TTFT from adapter merge. Should be <10% overhead.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 22
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-lora\"}[5m])))",
              "legendFormat": "LoRA"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 24,
          "type": "timeseries",
          "title": "GPU KV-Cache â€” LoRA vs Baseline",
          "description": "LoRA uses same base weights + small adapter. Cache usage should be SIMILAR to FP16.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 30
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-lora\"} * 100",
              "legendFormat": "LoRA"
            },
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-fp16\"} * 100",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "max": 100,
              "custom": {
                "fillOpacity": 15,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 25,
          "type": "timeseries",
          "title": "Decode Time per Token â€” LoRA vs Baseline",
          "description": "Per-token decode time. Should be nearly identical â€” LoRA doesn't change decode mechanics.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 30
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_per_output_token_seconds_bucket{app=\"mistral-7b-lora\"}[5m])))",
              "legendFormat": "LoRA"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_per_output_token_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 26,
          "type": "timeseries",
          "title": "Tokens Generated â€” LoRA vs Baseline",
          "description": "Cumulative token volume. Useful for A/B volume consistency checks.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 30
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:generation_tokens_total{app=\"mistral-7b-lora\"}",
              "legendFormat": "LoRA"
            },
            {
              "expr": "vllm:generation_tokens_total{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 Baseline"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 30,
          "type": "row",
          "title": "âš–ï¸ Evaluation â€” Judge Model Performance",
          "gridPos": {
            "h": 1,
            "w": 24,
            "x": 0,
            "y": 38
          },
          "collapsed": false,
          "panels": []
        },
        {
          "id": 31,
          "type": "timeseries",
          "title": "P95 E2E Latency â€” Judge",
          "description": "Judge scoring latency. Should stay under 3s for interactive scoring.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 39
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-judge\"}[5m])))",
              "legendFormat": "Judge P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 32,
          "type": "timeseries",
          "title": "Generation Throughput â€” Judge",
          "description": "Judge tokens/sec. Higher = faster scoring pipeline. Should be stable under load.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 39
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-judge\"}",
              "legendFormat": "Judge tok/s"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 33,
          "type": "timeseries",
          "title": "Time to First Token â€” Judge",
          "description": "Prefill time for scoring requests. Affected by prompt-response pair length.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 39
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-judge\"}[5m])))",
              "legendFormat": "Judge P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 34,
          "type": "timeseries",
          "title": "GPU KV-Cache â€” Judge",
          "description": "Judge GPU memory usage. If near 100%, scoring requests will queue.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 47
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-judge\"} * 100",
              "legendFormat": "Judge Cache %"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "max": 100,
              "custom": {
                "fillOpacity": 15,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 35,
          "type": "timeseries",
          "title": "Queue Depth â€” Judge",
          "description": "Active and queued requests. Rising 'waiting' line = judge can't keep up with scoring demand.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 47
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:num_requests_running{app=\"mistral-7b-judge\"}",
              "legendFormat": "Running"
            },
            {
              "expr": "vllm:num_requests_waiting{app=\"mistral-7b-judge\"}",
              "legendFormat": "Waiting"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 36,
          "type": "timeseries",
          "title": "Avg Tokens per Request â€” Judge",
          "description": "Scoring responses should be short (~50-100 tokens). High values suggest over-generation.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 47
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "rate(vllm:generation_tokens_total{app=\"mistral-7b-judge\"}[5m]) / rate(vllm:request_success_total{app=\"mistral-7b-judge\"}[5m])",
              "legendFormat": "Judge avg tok/req"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 40,
          "type": "row",
          "title": "ðŸ† Head-to-Head â€” All Models",
          "gridPos": {
            "h": 1,
            "w": 24,
            "x": 0,
            "y": 55
          },
          "collapsed": false,
          "panels": []
        },
        {
          "id": 41,
          "type": "timeseries",
          "title": "P95 Latency â€” All Models",
          "description": "All 4 models on one graph. Lower is better. AWQ should be lowest, LoRA â‰ˆ FP16.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 56
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-awq\"}[5m])))",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-lora\"}[5m])))",
              "legendFormat": "LoRA"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-judge\"}[5m])))",
              "legendFormat": "Judge"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 5,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 42,
          "type": "timeseries",
          "title": "Throughput â€” All Models",
          "description": "Generation throughput across all models. Higher is better. AWQ should lead.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 56
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-awq\"}",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 Baseline"
            },
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-lora\"}",
              "legendFormat": "LoRA"
            },
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-judge\"}",
              "legendFormat": "Judge"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 5,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 43,
          "type": "timeseries",
          "title": "GPU KV-Cache â€” All Models",
          "description": "GPU memory comparison. AWQ should be lowest, FP16/LoRA similar, Judge varies with load.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 56
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-awq\"} * 100",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-fp16\"} * 100",
              "legendFormat": "FP16 Baseline"
            },
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-lora\"} * 100",
              "legendFormat": "LoRA"
            },
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-judge\"} * 100",
              "legendFormat": "Judge"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "max": 100,
              "custom": {
                "fillOpacity": 5,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 44,
          "type": "timeseries",
          "title": "Request Rate â€” All Models",
          "description": "Per-model request rate. Shows traffic distribution during benchmark runs.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 64
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "sum by (app)(rate(vllm:request_success_total{app=\"mistral-7b-awq\"}[5m]))",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "sum by (app)(rate(vllm:request_success_total{app=\"mistral-7b-fp16\"}[5m]))",
              "legendFormat": "FP16 Baseline"
            },
            {
              "expr": "sum by (app)(rate(vllm:request_success_total{app=\"mistral-7b-lora\"}[5m]))",
              "legendFormat": "LoRA"
            },
            {
              "expr": "sum by (app)(rate(vllm:request_success_total{app=\"mistral-7b-judge\"}[5m]))",
              "legendFormat": "Judge"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "reqps",
              "custom": {
                "fillOpacity": 5,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 45,
          "type": "timeseries",
          "title": "TTFT â€” All Models",
          "description": "Time to first token across all models. AWQ should be fastest (smallest model).",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 64
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-awq\"}[5m])))",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 Baseline"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-lora\"}[5m])))",
              "legendFormat": "LoRA"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-judge\"}[5m])))",
              "legendFormat": "Judge"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 5,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 46,
          "type": "timeseries",
          "title": "Queue Depth â€” All Models",
          "description": "Requests running + waiting per model. Any model with rising 'waiting' needs attention.",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 64
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "vllm:num_requests_running{app=\"mistral-7b-awq\"}",
              "legendFormat": "AWQ running"
            },
            {
              "expr": "vllm:num_requests_running{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 running"
            },
            {
              "expr": "vllm:num_requests_running{app=\"mistral-7b-lora\"}",
              "legendFormat": "LoRA running"
            },
            {
              "expr": "vllm:num_requests_running{app=\"mistral-7b-judge\"}",
              "legendFormat": "Judge running"
            },
            {
              "expr": "vllm:num_requests_waiting{app=~\"mistral-7b-.*\"}",
              "legendFormat": "{{app}} waiting"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "custom": {
                "fillOpacity": 5,
                "lineWidth": 2
              }
            }
          }
        },
        {
          "id": 50,
          "type": "row",
          "title": "ðŸ“ Benchmark Traces & Logs",
          "gridPos": {
            "h": 1,
            "w": 24,
            "x": 0,
            "y": 72
          },
          "collapsed": false,
          "panels": []
        },
        {
          "id": 51,
          "type": "logs",
          "title": "Benchmark & Harness Logs",
          "description": "Gateway and harness log entries related to benchmark runs. Filter by team, run_id, or error.",
          "gridPos": {
            "h": 10,
            "w": 12,
            "x": 0,
            "y": 73
          },
          "datasource": "Loki",
          "targets": [
            {
              "expr": "{namespace=\"platform\"} |~ \"bench|benchmark|harness|predict\"",
              "legendFormat": ""
            }
          ],
          "options": {
            "showTime": true,
            "sortOrder": "Descending",
            "enableLogDetails": true
          }
        },
        {
          "id": 52,
          "type": "table",
          "title": "Request Volume by Team (1h)",
          "description": "Total gateway requests per team in the last hour. Confirms benchmark traffic reached each team's model.",
          "gridPos": {
            "h": 10,
            "w": 12,
            "x": 12,
            "y": 73
          },
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "sum by (team)(increase(lab_gateway_requests_total[1h]))",
              "legendFormat": "",
              "format": "table",
              "instant": true
            }
          ],
          "transformations": [
            {
              "id": "organize",
              "options": {
                "excludeByName": {
                  "Time": true
                },
                "renameByName": {
                  "team": "Team",
                  "Value": "Requests (1h)"
                }
              }
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short"
            }
          }
        }
      ],
      "refresh": "15s",
      "schemaVersion": 39,
      "tags": [
        "llm",
        "benchmark",
        "teams",
        "quantization",
        "finetuning",
        "evaluation",
        "kpi"
      ],
      "templating": {
        "list": []
      },
      "time": {
        "from": "now-1h",
        "to": "now"
      },
      "title": "Team Benchmark KPIs",
      "uid": "llm-platform-overview"
    }

  llm-operations.json: |
    {
      "annotations": { "list": [] },
      "editable": true,
      "fiscalYearStartMonth": 0,
      "graphTooltip": 0,
      "id": null,
      "links": [],
      "panels": [
        {
          "id": 1,
          "title": "LLM Platform Operations",
          "type": "llmplatform-ops-panel",
          "gridPos": { "h": 24, "w": 24, "x": 0, "y": 0 },
          "options": {
            "gatewayUrl": "/gateway-proxy"
          }
        }
      ],
      "refresh": "30s",
      "schemaVersion": 39,
      "tags": ["llm", "operations"],
      "templating": { "list": [] },
      "time": { "from": "now-1h", "to": "now" },
      "title": "LLM Operations Console",
      "uid": "llm-operations-console"
    }
