# k8s/base/observability/grafana-dashboards.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards-provider
  namespace: observability
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: observability
data:
  llm-platform.json: |
    {
      "annotations": {
        "list": []
      },
      "editable": true,
      "fiscalYearStartMonth": 0,
      "graphTooltip": 1,
      "id": null,
      "links": [],
      "panels": [
        {
          "id": 1,
          "title": "Team \u2192 Model Routing",
          "type": "text",
          "gridPos": {
            "h": 5,
            "w": 24,
            "x": 0,
            "y": 0
          },
          "options": {
            "mode": "markdown",
            "content": "| Team | Model Variant | Pod | Mission |\n|------|--------------|-----|---------|\n| **Quantization** | AWQ (4-bit) | `mistral-7b-awq` | Compare compressed vs full-precision inference quality |\n| **Fine-Tuning** | LoRA adapter | `mistral-7b-lora` | Domain-adapted model, A/B vs base |\n| **Evaluation** | Judge model | `mistral-7b-judge` | Score prompt-response pairs for coherence/factuality/toxicity |\n| _Reference_ | FP16 baseline | `mistral-7b-fp16` | Full-precision baseline for comparison |"
          }
        },
        {
          "id": 2,
          "title": "Total Request Rate",
          "type": "stat",
          "gridPos": {
            "h": 4,
            "w": 6,
            "x": 0,
            "y": 5
          },
          "targets": [
            {
              "expr": "sum(rate(vllm:request_success_total{app=~\"mistral-7b-.*\"}[5m]))",
              "legendFormat": "req/s"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "reqps",
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 10
                  },
                  {
                    "color": "red",
                    "value": 50
                  }
                ]
              }
            }
          }
        },
        {
          "id": 3,
          "title": "Running Requests",
          "type": "stat",
          "gridPos": {
            "h": 4,
            "w": 6,
            "x": 6,
            "y": 5
          },
          "targets": [
            {
              "expr": "sum(vllm:num_requests_running{app=~\"mistral-7b-.*\"})",
              "legendFormat": "running"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 5
                  },
                  {
                    "color": "red",
                    "value": 20
                  }
                ]
              }
            }
          }
        },
        {
          "id": 4,
          "title": "Waiting Requests",
          "type": "stat",
          "gridPos": {
            "h": 4,
            "w": 6,
            "x": 12,
            "y": 5
          },
          "targets": [
            {
              "expr": "sum(vllm:num_requests_waiting{app=~\"mistral-7b-.*\"})",
              "legendFormat": "waiting"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 1
                  },
                  {
                    "color": "red",
                    "value": 5
                  }
                ]
              }
            }
          }
        },
        {
          "id": 5,
          "title": "Active Models",
          "type": "stat",
          "gridPos": {
            "h": 4,
            "w": 6,
            "x": 18,
            "y": 5
          },
          "targets": [
            {
              "expr": "count(up{app=~\"mistral-7b-.*\"} == 1)",
              "legendFormat": "active"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "thresholds": {
                "steps": [
                  {
                    "color": "red",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 3
                  },
                  {
                    "color": "green",
                    "value": 4
                  }
                ]
              }
            }
          }
        },
        {
          "id": 6,
          "title": "\u26a1 Quantization: AWQ vs FP16 \u2014 Generation Throughput (tok/s)",
          "description": "Compare AWQ (4-bit) throughput against FP16 baseline. AWQ should be faster due to smaller model size.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 9
          },
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-awq\"}",
              "legendFormat": "AWQ (4-bit)"
            },
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 (baseline)"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "tok/s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 7,
          "title": "\u26a1 Quantization: AWQ vs FP16 \u2014 P95 Latency (s)",
          "description": "End-to-end request latency comparison. Lower is better. AWQ should have similar or lower latency.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 9
          },
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-awq\"}[5m])))",
              "legendFormat": "AWQ P95"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 8,
          "title": "\u26a1 Quantization: GPU Memory Savings",
          "description": "GPU KV-cache usage comparison. AWQ 4-bit should consume significantly less GPU memory than FP16.",
          "type": "bargauge",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 9
          },
          "targets": [
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-awq\"} * 100",
              "legendFormat": "AWQ GPU %"
            },
            {
              "expr": "vllm:gpu_cache_usage_perc{app=\"mistral-7b-fp16\"} * 100",
              "legendFormat": "FP16 GPU %"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "min": 0,
              "max": 100,
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 60
                  },
                  {
                    "color": "red",
                    "value": 85
                  }
                ]
              }
            }
          }
        },
        {
          "id": 9,
          "title": "\ud83d\udd27 Fine-Tuning: LoRA vs FP16 \u2014 Generation Throughput (tok/s)",
          "description": "Compare LoRA-adapted model throughput against FP16 baseline. LoRA adds adapter overhead.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 17
          },
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-lora\"}",
              "legendFormat": "LoRA"
            },
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-fp16\"}",
              "legendFormat": "FP16 (baseline)"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "tok/s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 10,
          "title": "\ud83d\udd27 Fine-Tuning: LoRA vs FP16 \u2014 P95 Latency (s)",
          "description": "End-to-end latency comparison. LoRA may add slight overhead from adapter layer computation.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 17
          },
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-lora\"}[5m])))",
              "legendFormat": "LoRA P95"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 11,
          "title": "\ud83d\udd27 Fine-Tuning: LoRA vs FP16 \u2014 Time to First Token (s)",
          "description": "TTFT measures prefill speed. Higher TTFT means longer wait before first token appears.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 17
          },
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-lora\"}[5m])))",
              "legendFormat": "LoRA TTFT P95"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-fp16\"}[5m])))",
              "legendFormat": "FP16 TTFT P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 12,
          "title": "\ud83d\udcca Evaluation: Judge Queue Depth",
          "description": "Running + Waiting requests on the judge model. Rising 'waiting' means scoring latency will increase.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 25
          },
          "targets": [
            {
              "expr": "vllm:num_requests_running{app=\"mistral-7b-judge\"}",
              "legendFormat": "Running"
            },
            {
              "expr": "vllm:num_requests_waiting{app=\"mistral-7b-judge\"}",
              "legendFormat": "Waiting"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "custom": {
                "fillOpacity": 20
              }
            }
          }
        },
        {
          "id": 13,
          "title": "\ud83d\udcca Evaluation: Judge Scoring Throughput",
          "description": "Tokens per second processed by the judge model. Higher = faster scoring.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 25
          },
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=\"mistral-7b-judge\"}",
              "legendFormat": "Judge tok/s"
            },
            {
              "expr": "vllm:avg_prompt_throughput_toks_per_s{app=\"mistral-7b-judge\"}",
              "legendFormat": "Judge prefill tok/s"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "tok/s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 14,
          "title": "\ud83d\udcca Evaluation: Judge P95 Latency (s)",
          "description": "How long each scoring request takes. Critical for batch evaluation throughput.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 25
          },
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:e2e_request_latency_seconds_bucket{app=\"mistral-7b-judge\"}[5m])))",
              "legendFormat": "Judge E2E P95"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_to_first_token_seconds_bucket{app=\"mistral-7b-judge\"}[5m])))",
              "legendFormat": "Judge TTFT P95"
            },
            {
              "expr": "histogram_quantile(0.95, sum by (le)(rate(vllm:time_per_output_token_seconds_bucket{app=\"mistral-7b-judge\"}[5m])))",
              "legendFormat": "Judge per-token P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 15,
          "title": "Request Rate by Model (req/s)",
          "description": "Per-model request rate \u2014 which team is generating the most traffic?",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 0,
            "y": 33
          },
          "targets": [
            {
              "expr": "sum by (app)(rate(vllm:request_success_total{app=~\"mistral-7b-.*\"}[5m]))",
              "legendFormat": "{{app}}"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "reqps",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 16,
          "title": "Generation Throughput \u2014 All Models (tok/s)",
          "description": "Side-by-side throughput comparison across all 4 models.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 12,
            "y": 33
          },
          "targets": [
            {
              "expr": "vllm:avg_generation_throughput_toks_per_s{app=~\"mistral-7b-.*\"}",
              "legendFormat": "{{app}}"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "tok/s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 17,
          "title": "Queue Depth by Model (running + waiting)",
          "description": "Running and waiting requests per model. Rising wait queues indicate capacity problems.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 0,
            "y": 41
          },
          "targets": [
            {
              "expr": "vllm:num_requests_running{app=~\"mistral-7b-.*\"}",
              "legendFormat": "{{app}} running"
            },
            {
              "expr": "vllm:num_requests_waiting{app=~\"mistral-7b-.*\"}",
              "legendFormat": "{{app}} waiting"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "custom": {
                "fillOpacity": 15
              }
            }
          }
        },
        {
          "id": 18,
          "title": "GPU KV-Cache Usage by Model (%)",
          "description": "GPU memory pressure per model. AWQ should use less than FP16. >85% = risk of OOM.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 12,
            "y": 41
          },
          "targets": [
            {
              "expr": "vllm:gpu_cache_usage_perc{app=~\"mistral-7b-.*\"} * 100",
              "legendFormat": "{{app}}"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "min": 0,
              "max": 100,
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 60
                  },
                  {
                    "color": "red",
                    "value": 85
                  }
                ]
              },
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 19,
          "title": "P95 E2E Latency \u2014 All Models (s)",
          "description": "Head-to-head latency comparison. Lower is better.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 0,
            "y": 49
          },
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le, app)(rate(vllm:e2e_request_latency_seconds_bucket{app=~\"mistral-7b-.*\"}[5m])))",
              "legendFormat": "{{app}} P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 20,
          "title": "Time to First Token \u2014 All Models (s)",
          "description": "Prefill time comparison. Affects user-perceived responsiveness.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 12,
            "y": 49
          },
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le, app)(rate(vllm:time_to_first_token_seconds_bucket{app=~\"mistral-7b-.*\"}[5m])))",
              "legendFormat": "{{app}} TTFT P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 21,
          "title": "Total Generation Tokens \u2014 All Models",
          "description": "Cumulative tokens generated. Shows which models are being used most heavily.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 0,
            "y": 57
          },
          "targets": [
            {
              "expr": "vllm:generation_tokens_total{app=~\"mistral-7b-.*\"}",
              "legendFormat": "{{app}}"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 22,
          "title": "Total Prompt (Prefill) Tokens \u2014 All Models",
          "description": "Cumulative prefill tokens. Shows input volume per model.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 8,
            "y": 57
          },
          "targets": [
            {
              "expr": "vllm:prompt_tokens_total{app=~\"mistral-7b-.*\"}",
              "legendFormat": "{{app}}"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "short",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 23,
          "title": "GPU KV-Cache \u2014 Instant Snapshot",
          "description": "Current GPU cache usage per model. Bar chart for quick comparison.",
          "type": "bargauge",
          "gridPos": {
            "h": 8,
            "w": 8,
            "x": 16,
            "y": 57
          },
          "targets": [
            {
              "expr": "vllm:gpu_cache_usage_perc{app=~\"mistral-7b-.*\"} * 100",
              "legendFormat": "{{app}}"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "min": 0,
              "max": 100,
              "thresholds": {
                "steps": [
                  {
                    "color": "green",
                    "value": null
                  },
                  {
                    "color": "yellow",
                    "value": 60
                  },
                  {
                    "color": "red",
                    "value": 85
                  }
                ]
              }
            }
          }
        },
        {
          "id": 24,
          "title": "Time Per Output Token \u2014 All Models (s)",
          "description": "How long each output token takes. Key metric for comparing model serving efficiency.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 0,
            "y": 65
          },
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum by (le, app)(rate(vllm:time_per_output_token_seconds_bucket{app=~\"mistral-7b-.*\"}[5m])))",
              "legendFormat": "{{app}} P95"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        },
        {
          "id": 25,
          "title": "Prefill Throughput \u2014 All Models (tok/s)",
          "description": "Prompt processing speed. Higher = faster initial processing of long prompts.",
          "type": "timeseries",
          "gridPos": {
            "h": 8,
            "w": 12,
            "x": 12,
            "y": 65
          },
          "targets": [
            {
              "expr": "vllm:avg_prompt_throughput_toks_per_s{app=~\"mistral-7b-.*\"}",
              "legendFormat": "{{app}}"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "tok/s",
              "custom": {
                "fillOpacity": 10
              }
            }
          }
        }
      ],
      "refresh": "10s",
      "schemaVersion": 39,
      "tags": [
        "llm",
        "platform",
        "teams",
        "quantization",
        "finetuning",
        "evaluation"
      ],
      "templating": {
        "list": []
      },
      "time": {
        "from": "now-1h",
        "to": "now"
      },
      "title": "LLM Platform Overview",
      "uid": "llm-platform-overview"
    }

  llm-operations.json: |
    {
      "annotations": { "list": [] },
      "editable": true,
      "fiscalYearStartMonth": 0,
      "graphTooltip": 0,
      "id": null,
      "links": [],
      "panels": [
        {
          "id": 1,
          "title": "LLM Platform Operations",
          "type": "llmplatform-ops-panel",
          "gridPos": { "h": 24, "w": 24, "x": 0, "y": 0 },
          "options": {
            "gatewayUrl": "/gateway-proxy"
          }
        }
      ],
      "refresh": "30s",
      "schemaVersion": 39,
      "tags": ["llm", "operations"],
      "templating": { "list": [] },
      "time": { "from": "now-1h", "to": "now" },
      "title": "LLM Operations Console",
      "uid": "llm-operations-console"
    }
