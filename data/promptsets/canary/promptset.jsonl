{"prompt_id": "canary-001", "prompt": "What is 2 + 2?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["4"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-002", "prompt": "What is 10 times 5?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["50"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-003", "prompt": "What is 100 divided by 4?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["25"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-004", "prompt": "What is the square root of 144?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["12"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-005", "prompt": "If x = 3 and y = 7, what is x + y?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["10"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-006", "prompt": "What is the capital of France?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Paris"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-007", "prompt": "What planet is closest to the Sun?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Mercury"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-008", "prompt": "Who wrote Romeo and Juliet?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Shakespeare"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-009", "prompt": "What is the chemical symbol for water?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["H2O"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-010", "prompt": "How many continents are there?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["7", "seven"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-011", "prompt": "Translate 'hello' to Spanish.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["hola", "Hola"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-012", "prompt": "Translate 'thank you' to French.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["merci", "Merci"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-013", "prompt": "Translate 'goodbye' to German.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Tsch\u00fcss", "Auf Wiedersehen", "tsch\u00fcss", "auf wiedersehen"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-014", "prompt": "Complete the sentence: The capital of France is", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Paris"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-015", "prompt": "Complete: Water freezes at", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["0", "32", "zero"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-016", "prompt": "Is the following a fruit or vegetable: apple", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["fruit", "Fruit"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-017", "prompt": "Is the number 7 odd or even?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["odd", "Odd"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-018", "prompt": "List three primary colors.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["red", "blue", "yellow"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-019", "prompt": "Name three planets in our solar system.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Earth"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-020", "prompt": "List three programming languages.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Python"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-021", "prompt": "Define 'photosynthesis' in one sentence.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["light", "plant", "energy"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-022", "prompt": "What is machine learning in one sentence?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["data", "learn"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-023", "prompt": "What is an API?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["interface", "application"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-024", "prompt": "What year did World War II end?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["1945"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-025", "prompt": "What is the boiling point of water in Celsius?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["100"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-026", "prompt": "How many days are in a leap year?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["366"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-027", "prompt": "What is the speed of light in km/s approximately?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["300", "299"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-028", "prompt": "Who painted the Mona Lisa?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Vinci", "Leonardo", "Da Vinci"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-029", "prompt": "What is the largest ocean on Earth?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Pacific"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-030", "prompt": "What gas do plants absorb from the atmosphere?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["CO2", "carbon dioxide", "Carbon dioxide"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-031", "prompt": "If a train travels 60 mph for 2 hours, how far does it go?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["120"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-032", "prompt": "If I have 3 apples and give away 1, how many do I have?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["2"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-033", "prompt": "What comes next in the pattern: 2, 4, 6, 8, ?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["10"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-034", "prompt": "What is the currency of Japan?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["yen", "Yen"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-035", "prompt": "What is DNA an abbreviation for?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["deoxyribonucleic"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-036", "prompt": "What is the smallest prime number?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["2"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-037", "prompt": "Name the author of 'A Brief History of Time'.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Hawking"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-038", "prompt": "What is the SI unit of force?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Newton", "newton"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-039", "prompt": "How many legs does a spider have?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["8", "eight"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-040", "prompt": "What is the freezing point of water in Fahrenheit?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["32"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-041", "prompt": "Spell the word 'necessary'.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["n-e-c-e-s-s-a-r-y", "necessary"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-042", "prompt": "What is the opposite of 'hot'?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["cold", "Cold"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-043", "prompt": "Convert 1 kilometer to meters.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["1000", "1,000"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-044", "prompt": "What is 15% of 200?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["30"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-045", "prompt": "What color do you get mixing red and blue?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["purple", "Purple", "violet", "Violet"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-046", "prompt": "Name the largest mammal.", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["whale", "Whale"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-047", "prompt": "What does CPU stand for?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["central processing unit", "Central Processing Unit", "Central processing unit"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-048", "prompt": "What is gravity measured in?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["m/s", "meter", "Newton"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-049", "prompt": "How many minutes are in an hour?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["60"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
{"prompt_id": "canary-050", "prompt": "What is the tallest mountain in the world?", "scenario_id": "canary-v1", "dataset_id": "canary-deployment-health", "expected_contains": ["Everest"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": null, "split": null, "metadata": null}
