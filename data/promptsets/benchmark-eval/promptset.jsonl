{"prompt_id": "be-coh-001", "prompt": "Write a structured explanation of how photosynthesis works.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-002", "prompt": "Explain the water cycle with clear transitions between stages.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-003", "prompt": "Describe three causes of the French Revolution in a logical order.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-004", "prompt": "Write a step-by-step guide to setting up a Python virtual environment.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-005", "prompt": "Explain how email works from sender to recipient.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-006", "prompt": "Describe the scientific method in sequential steps.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-007", "prompt": "Write a coherent paragraph explaining why the sky is blue.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-008", "prompt": "Explain the process of making bread from scratch.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-009", "prompt": "Describe how a bill becomes a law in the United States.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-010", "prompt": "Write a structured comparison of electric and gas cars.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-011", "prompt": "Explain the food chain with clear examples at each level.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-012", "prompt": "Describe the lifecycle of a butterfly in order.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-013", "prompt": "Write a logical argument for why exercise is important.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-014", "prompt": "Explain how the internet works in simple terms.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-015", "prompt": "Describe three branches of the US government and their roles.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-016", "prompt": "Write a structured explanation of supply and demand.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-017", "prompt": "Explain how a search engine indexes and retrieves web pages.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-018", "prompt": "Describe the process of human blood circulation.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-019", "prompt": "Write a step-by-step explanation of long division.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-020", "prompt": "Explain how vaccines provide immunity against diseases.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-021", "prompt": "Describe the nitrogen cycle in ecosystems.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-022", "prompt": "Write a coherent explanation of how WiFi works.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-023", "prompt": "Explain the difference between weather and climate.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-024", "prompt": "Describe how a car engine converts fuel to motion.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-025", "prompt": "Write a structured overview of the solar system.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-026", "prompt": "Explain how recycling works from collection to reuse.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-027", "prompt": "Describe the rock cycle with transitions between types.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-028", "prompt": "Write a logical explanation of compound interest.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-029", "prompt": "Explain how sound travels through different media.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-030", "prompt": "Describe the process of evolution by natural selection.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-031", "prompt": "Write a step-by-step guide to making a budget.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-032", "prompt": "Explain how nuclear power plants generate electricity.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-033", "prompt": "Describe the structure of the United Nations.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-034", "prompt": "Write a coherent explanation of how GPS works.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-035", "prompt": "Explain the greenhouse effect and its consequences.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-036", "prompt": "Describe how democracy differs from authoritarianism.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-037", "prompt": "Write a structured explanation of machine learning.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-038", "prompt": "Explain how tides work including the role of the Moon.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-039", "prompt": "Describe the process of protein synthesis in cells.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-040", "prompt": "Write a logical comparison of renewable energy sources.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-041", "prompt": "Explain how a computer boots from power-on to desktop.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-042", "prompt": "Describe the process of photovoltaic energy conversion.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-043", "prompt": "Write a structured overview of the digestive system.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-044", "prompt": "Explain how blockchain technology enables cryptocurrency.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-045", "prompt": "Describe the Krebs cycle in cellular respiration.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-046", "prompt": "Write a coherent explanation of how 3D printing works.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-047", "prompt": "Explain the principles of aerodynamics and flight.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-048", "prompt": "Describe how the human immune system responds to infection.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-049", "prompt": "Write a logical argument for space exploration funding.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-coh-050", "prompt": "Explain how fiber optic cables transmit data.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "coherence", "split": null, "metadata": null}
{"prompt_id": "be-help-001", "prompt": "I want to learn Python. Create a 4-week study plan for beginners.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-002", "prompt": "I need to choose between AWS, GCP, and Azure. Compare them for a startup.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-003", "prompt": "My Docker container keeps crashing. Give me a troubleshooting checklist.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-004", "prompt": "I need to prepare for a system design interview. What should I study?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-005", "prompt": "How do I set up monitoring for a production Kubernetes cluster?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-006", "prompt": "I want to migrate from monolith to microservices. What is the strategy?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-007", "prompt": "My PostgreSQL queries are slow. Give me an optimization checklist.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-008", "prompt": "How do I implement CI/CD for a team of 5 developers?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-009", "prompt": "I need to choose a message queue. Compare Kafka, RabbitMQ, and SQS.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-010", "prompt": "Create a security hardening checklist for a Linux web server.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-011", "prompt": "How do I debug a memory leak in a Node.js application?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-012", "prompt": "I need to design a REST API for an e-commerce platform. Give guidelines.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-013", "prompt": "What are the steps to deploy a machine learning model to production?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-014", "prompt": "I need to set up disaster recovery for an AWS application. How?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-015", "prompt": "Create a checklist for reviewing a pull request.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-016", "prompt": "How do I optimize a React application for performance?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-017", "prompt": "I need to choose between SQL and NoSQL for my project. Help me decide.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-018", "prompt": "What steps should I take to reduce my cloud bill by 30%?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-019", "prompt": "I want to implement feature flags. What are the options and best practices?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-020", "prompt": "How do I handle database migrations in a zero-downtime deployment?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-021", "prompt": "Create a runbook for responding to a production outage.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-022", "prompt": "I need to choose a frontend framework. Compare React, Vue, and Svelte.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-023", "prompt": "How do I implement rate limiting in an API gateway?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-024", "prompt": "I want to set up automated testing. Create a testing strategy.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-025", "prompt": "What are the best practices for managing secrets in Kubernetes?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-026", "prompt": "I need to design a notification system. What architecture should I use?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-027", "prompt": "How do I implement caching effectively in a web application?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-028", "prompt": "Create a checklist for launching a new microservice to production.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-029", "prompt": "I need to implement authentication. Compare JWT, OAuth, and session-based.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-030", "prompt": "How do I troubleshoot network connectivity issues in Kubernetes?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-031", "prompt": "I want to implement infrastructure as code. Where do I start?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-032", "prompt": "What are the steps to secure a REST API?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-033", "prompt": "How do I set up observability for a microservices architecture?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-034", "prompt": "I need to choose a container orchestration tool. Compare K8s, Nomad, ECS.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-035", "prompt": "Create a guide for writing effective technical documentation.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-036", "prompt": "How do I implement a data pipeline for real-time analytics?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-037", "prompt": "I need to scale my database. What are the options?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-038", "prompt": "What are the steps to implement a service mesh?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-039", "prompt": "How do I choose between gRPC and REST for microservices communication?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-040", "prompt": "Create a developer onboarding checklist for a new team member.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-041", "prompt": "I need to implement a search feature. Compare Elasticsearch, Typesense, and Meilisearch.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-042", "prompt": "How do I set up cross-region replication for high availability?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-043", "prompt": "What are the best practices for error handling in distributed systems?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-044", "prompt": "I need to implement a job queue. What are the options?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-045", "prompt": "How do I optimize container images for production?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-046", "prompt": "Create a guide for implementing clean architecture in Python.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-047", "prompt": "I need to implement logging best practices. What should I log and how?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-048", "prompt": "How do I design a multi-tenant SaaS application?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-049", "prompt": "What are the steps to implement blue-green deployments on Kubernetes?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-help-050", "prompt": "I need to choose a time-series database. Compare Prometheus, InfluxDB, and TimescaleDB.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "helpfulness", "split": null, "metadata": null}
{"prompt_id": "be-fact-001", "prompt": "When was the first Moon landing and who commanded the mission?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1969", "Armstrong"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-002", "prompt": "What is the distance from Earth to the Sun in astronomical units?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1", "AU"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-003", "prompt": "Who wrote the Communist Manifesto?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Marx", "Engels"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-004", "prompt": "What is the formula for kinetic energy?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1/2", "mv", "v^2"], "expected_format": null, "target_output_tokens": 150, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-005", "prompt": "What is the population of China approximately?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1.4", "billion"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-006", "prompt": "When was the Berlin Wall built and when did it fall?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1961", "1989"], "expected_format": null, "target_output_tokens": 150, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-007", "prompt": "What is the smallest country in the world by area?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Vatican"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-008", "prompt": "Who discovered the structure of DNA?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Watson", "Crick"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-009", "prompt": "What is the Heisenberg uncertainty principle?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["position", "momentum", "simultaneously"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-010", "prompt": "When did the Roman Empire fall?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["476", "5th"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-011", "prompt": "What is Planck's constant?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["6.626", "10^-34"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-012", "prompt": "Who invented the World Wide Web?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Berners-Lee", "Tim"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-013", "prompt": "What is the mathematical constant e approximately equal to?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["2.718"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-014", "prompt": "When was the Magna Carta signed?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1215"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-015", "prompt": "What is the tallest building in the world?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Burj Khalifa"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-016", "prompt": "Who formulated the three laws of motion?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Newton"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-017", "prompt": "What is the chemical formula for sulfuric acid?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["H2SO4"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-018", "prompt": "When was the United Nations founded?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1945"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-019", "prompt": "What is the deepest point in the ocean?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Mariana", "Challenger"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-020", "prompt": "Who painted the Sistine Chapel ceiling?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Michelangelo"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-021", "prompt": "What is the speed of sound at sea level?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["343", "340"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-022", "prompt": "When was the printing press invented?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1440", "1450", "15th"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-023", "prompt": "What is the Fibonacci sequence?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1", "1", "2", "3", "5"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-024", "prompt": "Who wrote Don Quixote?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Cervantes"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-025", "prompt": "What is the largest lake in Africa?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Victoria"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-026", "prompt": "When was the transistor invented?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1947"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-027", "prompt": "What is the definition of entropy in thermodynamics?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["disorder", "energy", "heat"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-028", "prompt": "Who was the first woman to win a Nobel Prize?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Curie", "Marie"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-029", "prompt": "What is the atomic mass of hydrogen?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1.008", "1"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-030", "prompt": "When did the Ottoman Empire end?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1922", "1923"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-031", "prompt": "What is the Doppler effect?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["frequency", "source", "observer"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-032", "prompt": "Who proposed the heliocentric model?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Copernicus"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-033", "prompt": "What is the GDP of the United States approximately?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["25", "trillion"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-034", "prompt": "When was penicillin discovered?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1928"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-035", "prompt": "What is Euler's identity?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["e^i", "pi", "+1=0", "=-1"], "expected_format": null, "target_output_tokens": 150, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-036", "prompt": "Who was the last pharaoh of Egypt?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Cleopatra"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-037", "prompt": "What is the melting point of iron in Celsius?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1538", "1535"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-038", "prompt": "When was the Suez Canal opened?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1869"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-039", "prompt": "What is the universal gas constant R?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["8.314"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-040", "prompt": "Who invented the telephone?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Bell"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-041", "prompt": "What is the surface area of Earth in square kilometers?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["510", "million"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-042", "prompt": "When was the first successful heart transplant?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1967"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-043", "prompt": "What is the charge of a proton in coulombs?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1.6", "10^-19"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-044", "prompt": "Who wrote The Origin of Species?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Darwin"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-045", "prompt": "What is the orbital period of Earth around the Sun?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["365", "days"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-046", "prompt": "When was the Internet protocol suite (TCP/IP) standardized?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1983"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-047", "prompt": "What is the density of water at 4 degrees Celsius?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1000", "1 g"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-048", "prompt": "Who developed the periodic table?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["Mendeleev"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-049", "prompt": "What is the Chandrasekhar limit?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1.4", "solar mass"], "expected_format": null, "target_output_tokens": 150, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-fact-050", "prompt": "When was the Hubble Space Telescope launched?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["1990"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "factuality", "split": null, "metadata": null}
{"prompt_id": "be-edge-001", "prompt": "Is a hot dog a sandwich? Give a reasoned argument.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-002", "prompt": "Explain why the number 0 is even.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-003", "prompt": "Is Pluto a planet? Explain the scientific debate.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-004", "prompt": "Can you prove that 1+1=2?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-005", "prompt": "Write a paradox and explain why it is paradoxical.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-006", "prompt": "Is mathematics discovered or invented? Argue both sides.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-007", "prompt": "Explain the Ship of Theseus problem.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-008", "prompt": "Is it possible to think about nothing? Explain.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-009", "prompt": "Are there more grains of sand on Earth or stars in the universe?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-010", "prompt": "Can an AI be creative? Argue for and against.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-011", "prompt": "Explain the trolley problem and why it has no clear answer.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-012", "prompt": "Is infinity a number? Explain.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-013", "prompt": "Can you hear silence? Discuss.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-014", "prompt": "Is zero positive, negative, or neither?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["neither"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-015", "prompt": "What happens when an unstoppable force meets an immovable object?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-016", "prompt": "Is the glass half full or half empty? Give an engineer's answer.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-017", "prompt": "Explain the grandfather paradox in time travel.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-018", "prompt": "Is it ethical to eat meat? Present both perspectives.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-019", "prompt": "Can a machine understand language or only simulate understanding?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-020", "prompt": "What color is a mirror?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-021", "prompt": "If you replace every part of a car, is it still the same car?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-022", "prompt": "Is there a difference between being alive and not being dead?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-023", "prompt": "Explain why we park in driveways and drive on parkways.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-024", "prompt": "Can something be both true and false at the same time?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-025", "prompt": "Is math the language of the universe or a human construct?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-026", "prompt": "What would happen if everyone on Earth jumped at the same time?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-027", "prompt": "Is the color you see as blue the same blue I see?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-028", "prompt": "Can you step in the same river twice?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-029", "prompt": "Is a person who has lost all their memories still the same person?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-030", "prompt": "Explain why a set of all sets cannot contain itself.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-031", "prompt": "What came first, the chicken or the egg? Give a scientific answer.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-032", "prompt": "Is free will compatible with a deterministic universe?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-033", "prompt": "Can you define consciousness? Why is it hard to define?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-034", "prompt": "Is it possible to have a language with only one word?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-035", "prompt": "Explain the Banach-Tarski paradox in simple terms.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-036", "prompt": "Can we know what we do not know?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-037", "prompt": "Is the absence of evidence evidence of absence?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-038", "prompt": "What is the sound of one hand clapping?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-039", "prompt": "Can a liar truthfully say they are lying?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-040", "prompt": "Is a copy of a masterpiece art?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-041", "prompt": "Explain the Fermi paradox.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-042", "prompt": "Is time travel theoretically possible? What does physics say?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-043", "prompt": "Can an infinite hotel always accommodate one more guest?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-044", "prompt": "What would the world look like if pi were exactly 3?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-045", "prompt": "Is there a largest prime number?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["no", "infinitely"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-046", "prompt": "Can you have a thought without language?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-047", "prompt": "Is simplicity always better in design?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-048", "prompt": "Explain the coastline paradox.", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-049", "prompt": "Is there a meaningful difference between 0.999... and 1?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": ["equal", "same", "no"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "edge_case", "split": null, "metadata": null}
{"prompt_id": "be-edge-050", "prompt": "Can artificial intelligence have emotions?", "scenario_id": "benchmark-eval-v1", "dataset_id": "benchmark-eval", "expected_contains": null, "expected_format": null, "target_output_tokens": 500, "bucket": "long", "category": "edge_case", "split": null, "metadata": null}
