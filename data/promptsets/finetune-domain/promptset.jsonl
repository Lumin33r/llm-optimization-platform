{"prompt_id": "ft-001", "prompt": "What is the first-line treatment for type 2 diabetes?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["metformin"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-002", "prompt": "What are the symptoms of myocardial infarction?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["chest", "pain"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-003", "prompt": "Explain the difference between Type 1 and Type 2 diabetes.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["insulin"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-004", "prompt": "What are the stages of chronic kidney disease?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["GFR", "stage"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-005", "prompt": "Describe the mechanism of action of ACE inhibitors.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["angiotensin", "enzyme"], "expected_format": null, "target_output_tokens": 250, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-006", "prompt": "What is the Glasgow Coma Scale and how is it used?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["eye", "verbal", "motor"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-007", "prompt": "List the warning signs of a stroke using the FAST acronym.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["face", "arm", "speech", "time"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-008", "prompt": "What are common side effects of statin medications?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["muscle"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-009", "prompt": "Explain what an A1C test measures and normal ranges.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["hemoglobin", "blood sugar", "glucose"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-010", "prompt": "What is the difference between an MRI and CT scan?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["magnetic", "radiation"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "medical", "split": null, "metadata": null}
{"prompt_id": "ft-011", "prompt": "What is the difference between civil and criminal law?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["civil", "criminal"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-012", "prompt": "Explain the concept of habeas corpus.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["detention", "court", "unlawful"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-013", "prompt": "What is the doctrine of stare decisis?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["precedent"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-014", "prompt": "Define 'burden of proof' in a legal context.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["evidence", "prove"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-015", "prompt": "What are the elements of a valid contract?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["offer", "acceptance", "consideration"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-016", "prompt": "Explain the difference between a felony and a misdemeanor.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["serious", "punishment"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-017", "prompt": "What is the Miranda warning and when must it be given?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["right", "silent", "attorney"], "expected_format": null, "target_output_tokens": 250, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-018", "prompt": "Define 'tort' in legal terms and give an example.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["harm", "civil", "wrong"], "expected_format": null, "target_output_tokens": 200, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-019", "prompt": "What is the difference between patent and copyright?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["invention", "original work", "protect"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-020", "prompt": "Explain what 'due process' means under the 14th Amendment.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["fair", "law", "rights"], "expected_format": null, "target_output_tokens": 250, "bucket": "medium", "category": "legal", "split": null, "metadata": null}
{"prompt_id": "ft-021", "prompt": "Explain Kubernetes pod scheduling and affinity rules.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["node", "affinity", "schedule"], "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "code", "split": null, "metadata": null}
{"prompt_id": "ft-022", "prompt": "Describe how a B-tree index works in a database.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["tree", "node", "key"], "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "code", "split": null, "metadata": null}
{"prompt_id": "ft-023", "prompt": "Explain the difference between optimistic and pessimistic concurrency control.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["lock", "conflict", "transaction"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "code", "split": null, "metadata": null}
{"prompt_id": "ft-024", "prompt": "Describe the Raft consensus algorithm.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["leader", "election", "log"], "expected_format": null, "target_output_tokens": 400, "bucket": "long", "category": "code", "split": null, "metadata": null}
{"prompt_id": "ft-025", "prompt": "Explain how gRPC differs from REST APIs.", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["protocol buffer", "HTTP/2", "binary"], "expected_format": null, "target_output_tokens": 300, "bucket": "medium", "category": "code", "split": null, "metadata": null}
{"prompt_id": "ft-026", "prompt": "What is 15 * 17?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["255"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": "math", "split": null, "metadata": null}
{"prompt_id": "ft-027", "prompt": "What is the capital of Japan?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["Tokyo"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": "knowledge", "split": null, "metadata": null}
{"prompt_id": "ft-028", "prompt": "Who discovered penicillin?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["Fleming"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "knowledge", "split": null, "metadata": null}
{"prompt_id": "ft-029", "prompt": "What year did the Berlin Wall fall?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["1989"], "expected_format": null, "target_output_tokens": 50, "bucket": "short", "category": "knowledge", "split": null, "metadata": null}
{"prompt_id": "ft-030", "prompt": "What is the chemical formula for glucose?", "scenario_id": "finetune-domain-v1", "dataset_id": "finetune-domain", "expected_contains": ["C6H12O6"], "expected_format": null, "target_output_tokens": 100, "bucket": "medium", "category": "knowledge", "split": null, "metadata": null}
